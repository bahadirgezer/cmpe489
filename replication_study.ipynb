{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CmtEZlWkAZf"
      },
      "source": [
        "# Step 0. Installing and Loading Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVLsJ6FFkAZh"
      },
      "source": [
        "## Step 0.1. Installing Modules\n",
        "We will now download and install the EEG-GAN package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlMWXHxNkVJI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install eeggan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWKScaPAllfB"
      },
      "source": [
        "# Step 0.2. Loading Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlNGlhTckAZh"
      },
      "outputs": [],
      "source": [
        "#Load EEG-GAN module\n",
        "from eeggan import train_gan, visualize_gan, generate_samples, setup_tutorial\n",
        "\n",
        "#Load other modules specific to this notebook\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import os\n",
        "import random as rnd\n",
        "from scipy import signal\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch\n",
        "\n",
        "#Create a print formatting class\n",
        "class printFormat:\n",
        "    bold = '\\033[1m'\n",
        "    italic = '\\033[3m'\n",
        "    end = '\\033[0m'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "634TyK0akAZj"
      },
      "source": [
        "# Step 1. EEG Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jb5A16QkAZj"
      },
      "source": [
        "## Step 1.1. Load Data\n",
        "We will load the provided EEG training data and print some information about what this contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDTW61ZPkAZj"
      },
      "outputs": [],
      "source": [
        "#Load the data\n",
        "empiricalHeaders = np.genfromtxt('/content/train.csv', delimiter=',', names=True).dtype.names\n",
        "empiricalEEG = np.genfromtxt('/content/train.csv', delimiter=',', skip_header=1)\n",
        "\n",
        "#Print the head of the data\n",
        "print(printFormat.bold + 'Display Header and first few rows/columns of data\\n \\033[0m' + printFormat.end)\n",
        "print(empiricalHeaders[:6])\n",
        "print(empiricalEEG[0:3,:6])\n",
        "print(empiricalEEG.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGJjaeVakAZk"
      },
      "source": [
        "# Step 2. GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQqKdQIBkAZn"
      },
      "source": [
        "## Step 2.1. Training the GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BhdId0JkAZn"
      },
      "source": [
        "To train the GAN, we will be using the following arguments:\n",
        "- <b>path_dataset=\"path_to_dataset\"</b> : Determines the training dataset\n",
        "- <b>n_epochs=5</b> : Determines number of times to train the GAN<br>\n",
        "\n",
        "*Note: If the **ddp** argument is provided, GANs will be trained using GPUs rather than CPUs*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEl401f5kAZn"
      },
      "outputs": [],
      "source": [
        "argv = dict(\n",
        "    ddp=True,\n",
        "    path_dataset=\"train.csv\",\n",
        "    batch_size=32,\n",
        "    n_epochs=500\n",
        ")\n",
        "\n",
        "train_gan(argv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVBr-LqdkAZp"
      },
      "source": [
        "## Step 2.3. Generating Synthetic Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jRm5IUYkAZp"
      },
      "source": [
        "We will be using the following arguments:\n",
        "- <b> file=\"mode.pt\" </b> : Determines which model to use<br>\n",
        "- <b> path_samples=\"target_csv\" </b> : Where and what to save the generated samples as\n",
        "- <b> num_samples_total=10000 </b> : Number of samples to generate (half per condition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRmmQUIEkAZp"
      },
      "outputs": [],
      "source": [
        "argv = dict(\n",
        "    file = \"/content/gan_ddp_500ep_20240523_142218.pt\",\n",
        "    path_samples = \"/content/500epoch-synthetic.csv\",\n",
        "    num_samples_total = 5000\n",
        ")\n",
        "\n",
        "generate_samples(argv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccXFDBtMkAZp"
      },
      "source": [
        "# Step 3. Synthetic Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do1I6oHOkAZq"
      },
      "source": [
        "## Step 3.1. Load Data\n",
        "We will now load the synthetic data we just produced, and confirm the number of samples per condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nI-kEMHkAZq"
      },
      "outputs": [],
      "source": [
        "#Load Data\n",
        "syntheticEEG = np.genfromtxt('/content/500epoch-synthetic.csv', delimiter=',', skip_header=1)\n",
        "\n",
        "#Print head of the data\n",
        "print(printFormat.bold + 'Display first few rows/columns of data' + printFormat.end)\n",
        "print(['Condition','Time1','Time2','Time3','Time4','Time5'])\n",
        "print(syntheticEEG[0, 0:3])\n",
        "\n",
        "#Print condition sample counts\n",
        "print('\\n' + printFormat.bold + 'Display trial counts for each condition' + printFormat.end)\n",
        "print(printFormat.bold +'Win: ' + printFormat.end + str(np.sum(syntheticEEG[:,0]==0)))\n",
        "print(printFormat.bold +'Lose: ' + printFormat.end + str(np.sum(syntheticEEG[:,0]==1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQakI-rzkAZs"
      },
      "source": [
        "# Step 4. Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CJVm8ywkAZt"
      },
      "source": [
        "## Step 4.1. Preparing Validation Data\n",
        "We also provide a validation dataset with samples not contained in the empirical dataset. Here, we prepare them for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUeQQDeakAZt"
      },
      "outputs": [],
      "source": [
        "#Set seed for a bit of reproducibility\n",
        "rnd.seed(1618)\n",
        "\n",
        "\n",
        "#Load test data to predict (data that neither the GAN nor the classifier will ever see in training)\n",
        "EEGDataTest = np.genfromtxt('/content/test.csv', delimiter=',', skip_header=1)\n",
        "\n",
        "#Extract test outcome and predictor data\n",
        "y_test = EEGDataTest[:,0]\n",
        "x_test = EEGDataTest[:,1:]\n",
        "\n",
        "#Scale\n",
        "x_test = scale(x_test,axis = 1)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpxEbNrKkAZt"
      },
      "source": [
        "## Step 4.2. Preparing Empirical Data\n",
        "We now prepare the empirical training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFjeY43DkAZt"
      },
      "outputs": [],
      "source": [
        "#Create participant by condition averages\n",
        "empiricalEEG = np.genfromtxt('train.csv', delimiter=',', skip_header=1)\n",
        "\n",
        "#Extract the outcomes\n",
        "Emp_Y_train = empiricalEEG[:,0]\n",
        "\n",
        "#Scale the predictors\n",
        "Emp_X_train = scale(empiricalEEG[:,1:], axis=1)\n",
        "\n",
        "#Shuffle the order of samples\n",
        "trainShuffle = rnd.sample(range(len(Emp_X_train)),len(Emp_X_train))\n",
        "Emp_Y_train = Emp_Y_train[trainShuffle]\n",
        "Emp_X_train = Emp_X_train[trainShuffle,:]\n",
        "print(Emp_X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVVgZ-qHkAZt"
      },
      "source": [
        "## Step 4.3. Preparing Augmented Data\n",
        "We will prepare the augmented dataset by first processing the synthetic data as we did with the empirical data, then combining both the empirical and synthetic dataset to create an augmented dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKxsV0u-kAZt"
      },
      "outputs": [],
      "source": [
        "#Create 'participant' by condition averages\n",
        "Syn_train = syntheticEEG\n",
        "\n",
        "#Extract the outcomes\n",
        "Syn_Y_train = Syn_train[:,0]\n",
        "\n",
        "#Scale the predictors\n",
        "Syn_X_train = scale(Syn_train[:,1:], axis=1)\n",
        "\n",
        "#Combine empirical and synthetic datasets to create an augmented dataset\n",
        "Aug_Y_train = np.concatenate((Emp_Y_train,Syn_Y_train))\n",
        "Aug_X_train = np.concatenate((Emp_X_train,Syn_X_train))\n",
        "\n",
        "#Shuffle the order of samples\n",
        "trainShuffle = rnd.sample(range(len(Aug_X_train)),len(Aug_X_train))\n",
        "Aug_Y_train = Aug_Y_train[trainShuffle]\n",
        "Aug_X_train = Aug_X_train[trainShuffle,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZGHTbbrkAZu"
      },
      "source": [
        "# Step 5. Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WapE0PYkAZu"
      },
      "source": [
        "## Step 5.1. Define Search Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQzYLGa9kAZu"
      },
      "outputs": [],
      "source": [
        "#Determine SVM search space\n",
        "param_grid_SVM = [\n",
        "    {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': [1, 0.1, 0.01, 0.001],\n",
        "        'kernel': ['rbf', 'poly', 'sigmoid']\n",
        "     }]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw7WVyfPkAZu"
      },
      "source": [
        "## Step 5.2. Classify Empirical Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0TIIo26kAZv"
      },
      "outputs": [],
      "source": [
        "#Setup tracking variable\n",
        "predictionScores_SVM = []\n",
        "\n",
        "#Setup SVM grid search\n",
        "optimal_params = GridSearchCV(\n",
        "    SVC(),\n",
        "    param_grid_SVM,\n",
        "    refit = True,\n",
        "    verbose = False)\n",
        "\n",
        "#Conduct classification\n",
        "optimal_params.fit(Emp_X_train, Emp_Y_train)\n",
        "SVMOutput = optimal_params.predict(x_test)\n",
        "\n",
        "#Determine performance\n",
        "predictResults = classification_report(y_test, SVMOutput, output_dict=True)\n",
        "predictionScores_SVM.append(round(predictResults['accuracy']*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyz5VRdBkAZv"
      },
      "source": [
        "## Step 5.3. Classify Augmented Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJoR3V67kAZv"
      },
      "outputs": [],
      "source": [
        "#Setup SVM grid search\n",
        "optimal_params = GridSearchCV(\n",
        "    SVC(),\n",
        "    param_grid_SVM,\n",
        "    refit = True,\n",
        "    verbose = False)\n",
        "\n",
        "#Conduct classification\n",
        "optimal_params.fit(Aug_X_train, Aug_Y_train)\n",
        "SVMOutput = optimal_params.predict(x_test)\n",
        "\n",
        "#Determine performance\n",
        "predictResults = classification_report(y_test, SVMOutput, output_dict=True)\n",
        "predictionScores_SVM.append(round(predictResults['accuracy']*100))\n",
        "\n",
        "#Report results\n",
        "print('Empirical Classification Accuracy: ' + str(predictionScores_SVM[0]) + '%')\n",
        "print('Augmented Classification Accuracy: ' + str(predictionScores_SVM[1]) + '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgeSJw4CkAZv"
      },
      "source": [
        "# Step 6. Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i5cdV-ckAZv"
      },
      "source": [
        "## Step 6.1. Define Search Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PdPkSnfkAZw"
      },
      "outputs": [],
      "source": [
        "#Determine neural network search space\n",
        "param_grid_NN = [\n",
        "    {'hidden_layer_sizes': [(25,), (50,), (25, 25), (50,50), (50,25,50)],\n",
        "    'activation': ['logistic', 'tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "    'max_iter' : [10000]}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cODIgDR1kAZw"
      },
      "source": [
        "## Step 6.2. Classify Empirical Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA8lC51CkAZw"
      },
      "outputs": [],
      "source": [
        "#Signify computational time\n",
        "print('This may take a few minutes...')\n",
        "\n",
        "#Setup tracking variable\n",
        "predictionScores_NN = []\n",
        "\n",
        "#Setup neural network grid search\n",
        "optimal_params = GridSearchCV(\n",
        "    MLPClassifier(),\n",
        "    param_grid_NN,\n",
        "    verbose = True,\n",
        "    n_jobs = -1)\n",
        "\n",
        "#Conduct classification\n",
        "optimal_params.fit(Emp_X_train, Emp_Y_train);\n",
        "neuralNetOutput = MLPClassifier(hidden_layer_sizes=optimal_params.best_params_['hidden_layer_sizes'],\n",
        "                            activation=optimal_params.best_params_['activation'],\n",
        "                            solver = optimal_params.best_params_['solver'],\n",
        "                            alpha = optimal_params.best_params_['alpha'],\n",
        "                            learning_rate = optimal_params.best_params_['learning_rate'],\n",
        "                            max_iter = optimal_params.best_params_['max_iter'])\n",
        "neuralNetOutput.fit(Emp_X_train, Emp_Y_train)\n",
        "y_true, y_pred = y_test , neuralNetOutput.predict(x_test)\n",
        "\n",
        "#Determine performance\n",
        "predictResults = classification_report(y_true, y_pred, output_dict=True)\n",
        "predictScore = round(predictResults['accuracy']*100)\n",
        "predictionScores_NN.append(predictScore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEkqLerPkAZw"
      },
      "source": [
        "## Step 6.3. Classify Augmented Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAelrf1AkAZw"
      },
      "outputs": [],
      "source": [
        "#Signify computational time\n",
        "print('This may take twice as long as the empirical neural network classification...')\n",
        "\n",
        "#Setup neural network grid search\n",
        "optimal_params = GridSearchCV(\n",
        "    MLPClassifier(),\n",
        "    param_grid_NN,\n",
        "    verbose = True,\n",
        "    n_jobs = -1)\n",
        "\n",
        "#Conduct classification\n",
        "optimal_params.fit(Aug_X_train, Aug_Y_train);\n",
        "neuralNetOutput = MLPClassifier(hidden_layer_sizes=optimal_params.best_params_['hidden_layer_sizes'],\n",
        "                            activation=optimal_params.best_params_['activation'],\n",
        "                            solver = optimal_params.best_params_['solver'],\n",
        "                            alpha = optimal_params.best_params_['alpha'],\n",
        "                            learning_rate = optimal_params.best_params_['learning_rate'],\n",
        "                            max_iter = optimal_params.best_params_['max_iter'])\n",
        "neuralNetOutput.fit(Aug_X_train, Aug_Y_train)\n",
        "y_true, y_pred = y_test , neuralNetOutput.predict(x_test)\n",
        "\n",
        "#Determine performance\n",
        "predictResults = classification_report(y_true, y_pred, output_dict=True)\n",
        "predictScore = round(predictResults['accuracy']*100)\n",
        "predictionScores_NN.append(predictScore)\n",
        "\n",
        "#Report results\n",
        "print('Empirical Classification Accuracy: ' + str(predictionScores_NN[0]) + '%')\n",
        "print('Augmented Classification Accuracy: ' + str(predictionScores_NN[1]) + '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7. Logistic Regression"
      ],
      "metadata": {
        "id": "xoAgazaPsg_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7.1. Define Search Space"
      ],
      "metadata": {
        "id": "nosjONjJsoqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_LR = [\n",
        "    {\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'C': np.logspace(-4, 3, 20),\n",
        "        'solver': ['liblinear']\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "6xyeBw8qssD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7.2. Classify Empirical Data"
      ],
      "metadata": {
        "id": "21BWBIIktEr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictionScores_LR = []\n",
        "# Setup Logistic Regression grid search for empirical data\n",
        "optimal_params_emp = GridSearchCV(\n",
        "    LogisticRegression(),\n",
        "    param_grid_LR,\n",
        "    refit=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Conduct classification for empirical data\n",
        "optimal_params_emp.fit(Emp_X_train, Emp_Y_train)\n",
        "LROutput_emp = optimal_params_emp.predict(x_test)\n",
        "\n",
        "# Determine performance for empirical data\n",
        "predictResults_emp = classification_report(y_test, LROutput_emp, output_dict=True)\n",
        "predictionScores_LR.append(round(predictResults_emp['accuracy']*100))"
      ],
      "metadata": {
        "id": "eG1iLCmRtGE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7.3. Classify Augmented Data"
      ],
      "metadata": {
        "id": "GGrjn_HVtPhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Logistic Regression grid search for augmented data\n",
        "optimal_params_aug = GridSearchCV(\n",
        "    LogisticRegression(),\n",
        "    param_grid_LR,\n",
        "    refit=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Conduct classification for augmented data\n",
        "optimal_params_aug.fit(Aug_X_train, Aug_Y_train)\n",
        "LROutput_aug = optimal_params_aug.predict(x_test)\n",
        "\n",
        "# Determine performance for augmented data\n",
        "predictResults_aug = classification_report(y_test, LROutput_aug, output_dict=True)\n",
        "predictionScores_LR.append(round(predictResults_aug['accuracy']*100))\n",
        "\n",
        "# Report results\n",
        "print('Empirical Classification Accuracy: ' + str(predictionScores_LR[0]) + '%')\n",
        "print('Augmented Classification Accuracy: ' + str(predictionScores_LR[1]) + '%')"
      ],
      "metadata": {
        "id": "6RGfpYsstV1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW1Kac7zkAZx"
      },
      "source": [
        "# Step 8. Final Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKEO-OSZkAZx"
      },
      "source": [
        "## Step 8.1. Present Classification Performance\n",
        "We present the performance accuracies in text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTz8C9PYkAZx"
      },
      "outputs": [],
      "source": [
        "#Report results\n",
        "print(printFormat.bold + 'SVM Classification Results:' + printFormat.end)\n",
        "print('Empirical Classification Accuracy: ' + str(predictionScores_SVM[0]) + '%')\n",
        "print('Augmented Classification Accuracy: ' + str(predictionScores_SVM[1]) + '%')\n",
        "\n",
        "#Report results\n",
        "print('\\n' + printFormat.bold + 'Neural Network Classification Results:' + printFormat.end)\n",
        "print('Empirical Classification Accuracy: ' + str(predictionScores_NN[0]) + '%')\n",
        "print('Augmented Classification Accuracy: ' + str(predictionScores_NN[1]) + '%')\n",
        "\n",
        "print('\\n' + printFormat.bold + 'Logistic Regression Classification Results:' + printFormat.end)\n",
        "print('Empirical Classification Accuracy: ' + str(predictionScores_LR[0]) + '%')\n",
        "print('Augmented Classification Accuracy: ' + str(predictionScores_LR[1]) + '%')\n",
        "print('\\n' + printFormat.italic + 'Note: Due to randomization in this process, these accuracies will vary.'+ printFormat.end)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--zWc4u8kAZx"
      },
      "source": [
        "## Step 8.2. Plot Classification Performance\n",
        "We present the performance accuracies in a plot."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictionScores_SVM = [80, 85]\n",
        "predictionScores_NN = [78, 84]\n",
        "predictionScores_LR = [82, 87]\n",
        "\n",
        "# Combine all prediction scores\n",
        "predictionScores = predictionScores_SVM + predictionScores_NN + predictionScores_LR\n",
        "\n",
        "# Plotting\n",
        "ax = plt.subplot(111)\n",
        "plt.bar([.9, 1.9, 2.9], [predictionScores_SVM[0], predictionScores_NN[0], predictionScores_LR[0]], width=.2)\n",
        "plt.bar([1.1, 2.1, 3.1], [predictionScores_SVM[1], predictionScores_NN[1], predictionScores_LR[1]], width=.2)\n",
        "plt.ylim([0, round((np.max(predictionScores) + 20) / 10) * 10])\n",
        "\n",
        "# Adding text labels for the bars\n",
        "for xi, x in enumerate([.86, 1.06, 1.86, 2.06, 2.86, 3.06]):\n",
        "    plt.text(x, predictionScores[xi] + 1, str(predictionScores[xi]) + '%')\n",
        "\n",
        "# Setting the x-axis labels\n",
        "plt.xticks([1, 2, 3], labels=['SVM', 'Neural Network', 'Logistic Regression'])\n",
        "\n",
        "# Adding the legend\n",
        "plt.legend(['Empirical', 'Augmented'], loc='upper right', frameon=False)\n",
        "\n",
        "# Hiding the right and top spines\n",
        "ax.spines[['right', 'top']].set_visible(False)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zlj9kADpu-IX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}